import numpy as np
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pandas as pd
import Dict, List, Tuple

class EvaluationOfDocumentFeatureExtractor:
    def __init__(self, ref_vectors: Dict[str, np.ndarray]):        
        self.ref_vectors = ref_vectors
        self.ref_types = list(ref_vectors.keys())
        self.ref_matrix = np.array(list(ref_vectors.values()))
        
    def classify_doc(self, feature_vec: np.ndarray) -> Tuple[str, float]:                
        sim = cosine_similarity([feature_vec], self.ref_matrix)[0]      
        most_similar_idx = np.argmax(sim)
        confidence = sim[most_similar_idx]
        
        return self.ref_types[most_similar_idx], confidence
    
    def classify_docs(self, feature_vecs: List[np.ndarray]) -> pd.DataFrame:
        results = []
        for vector in feature_vecs:
            doc_type, confidence = self.classify_doc(vector)
            results.append({"predicted_type": doc_type, "confidence": confidence})
        
        return pd.DataFrame(results)
    
    def evaluate_clustering_quality(self, feature_vecs: List[np.ndarray], 
                                   predicted_labels: List[str]) -> Dict[str, float]:                
        unique_labels = list(set(predicted_labels))
        label_to_id = {label: i for i, label in enumerate(unique_labels)}
        numeric_labels = [label_to_id[label] for label in predicted_labels]
                
        feature_matrix = np.array(feature_vecs)
        
        metrics = {
            "silhouette_score": silhouette_score(feature_matrix, numeric_labels, metric='cosine')    
        }
                
        intra_class_distances = []
        inter_class_distances = []
        
        for i, (vec_i, label_i) in enumerate(zip(feature_vecs, predicted_labels)):
            for j, (vec_j, label_j) in enumerate(zip(feature_vecs, predicted_labels)):
                if i == j:
                    continue
                    
                similarity = cosine_similarity([vec_i], [vec_j])[0][0]
                distance = 1 - similarity
                
                if label_i == label_j:
                    intra_class_distances.append(distance)
                else:
                    inter_class_distances.append(distance)
        
        metrics["intra_class_mean_distance"] = np.mean(intra_class_distances)
        metrics["inter_class_mean_distance"] = np.mean(inter_class_distances)
        metrics["separation_ratio"] = metrics["inter_class_mean_distance"] / metrics["intra_class_mean_distance"]
        
        return metrics
    
    def visualize_confidence_distribution(self, confidence_scores: List[float], 
                                         predicted_types: List[str]) -> None:       
        plt.figure(figsize=(12, 6))                
        plt.subplot(1, 2, 1)
        plt.hist(confidence_scores, bins=20, alpha=0.7)
        plt.title("Overall Confidence Distribution")
        plt.xlabel("Confidence Score")
        plt.ylabel("Count")
                
        plt.subplot(1, 2, 2)
        df = pd.DataFrame({"confidence": confidence_scores, "type": predicted_types})
        top_types = df.groupby("type").size().nlargest(10).index
        
        boxplot_data = [df[df["type"] == t]["confidence"] for t in top_types]
        plt.boxplot(boxplot_data, labels=top_types, vert=False)
        plt.title("Confidence by Document Type (Top 10)")
        plt.xlabel("Confidence Score")
        plt.tight_layout()
        plt.savefig("confidence_distribution.png")
        
    def generate_evaluation_report(self, feature_vecs: List[np.ndarray]) -> Dict:
        classification_results = self.classify_docs(feature_vecs)          
        predicted_types = classification_results["predicted_type"].tolist()
        confidence_scores = classification_results["confidence"].tolist()
            
        clustering_metrics = self.evaluate_clustering_quality(feature_vecs, predicted_types)
              
        self.visualize_confidence_distribution(confidence_scores, predicted_types)
                
        report = {
            "classification_summary": {
                "total_documents": len(feature_vecs),
                "unique_document_types": len(set(predicted_types)),
                "type_distribution": classification_results["predicted_type"].value_counts().to_dict(),
                "mean_confidence": np.mean(confidence_scores),
                "low_confidence_count": sum(1 for score in confidence_scores if score < 0.7)
            },
            "clustering_metrics": clustering_metrics
        }
        
        return report
